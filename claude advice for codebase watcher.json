import anthropic

client = anthropic.Anthropic(
    # defaults to os.environ.get("ANTHROPIC_API_KEY")
    api_key="my_api_key",
)
message = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=4000,
    temperature=0,
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Modify the provided code and provide the new full code file plus an extra code file to implement the described changes.\n\nCHANGE DESCRIPTION:\nNotes for the design of the codebase watching function for the ChatGPTCLI application.The function will be that when the user types forward slash uniform escape enter into the prompt it will check to see whether any files in any of the codebases that it is watching have changed and for files that have been updated or added to the codebasesand which have the relevant file extensions it will add those to the beginning of the next message to ChatGPT and it will tell the user that it is going to do so just like at the beginning and it will tell the userwhich files have been added and which files have been modified and when they were added and when they were modifiedand actually not when they were added but just when they were modifiedand then the implementation will be structured like this making everything as pure and functional as possible there will be a class of object called a codebase state and the codebase state will be transformed each time the codebase state is checked but this will be treated as an immutable object so each time it will be copied into a mutated version instead of mutated in place and the function that identifies the codebase changes will take as an input a codebase state it will use io to look at the codebase and then it will producethe following outputs the first output will be a instance of the type codebase transformationwhich will consist of a set of the following types of changes additions deletions and updates the additions are simply relative paths thedeletions are also relative paths and the updates are a product type consisting of a relative path and a combined date time valueso the function no i should say the method which looks at the codebase so it takes first as the first uh so the parameter that it takes is the no it takes two parameters one is the codebase location second one is the list of file extensions to look at third one is the codebase state and it assumes that all the files listed in the codebase state match the criteria for inclusion based on the file extensions that is an assumption about the inputit outputs the following one is thecodebase transformation value representing all of the changes to the codebase state and then another one is a set of pairs of first of all the relative path of the file and secondlythe new contents of the file the items in this set represent all of the files which were either addedor modified and identified as such when the method was executedi think those should be the only two outputs of this method and then is a helper method to a higher level method which is also dependent on io via the helper method and the higher level method takes as an input the first the codebase location and secondly the codebase state and it runs the codebase checker function and then it uses the codebase transformation value in two ways first of all it applies that transformation to the codebase state object using a helper function from the codebase state class and then it also it turns that codebase transformation object into a text description using a helper function from the codebase state class and it returns this text description of the transfer of the changes observed along with the set of files that have changed but it returns a set of changes as a single string formatted as follows for each entry first there is a line saying contents of the file and then the relative file path and then a new line and followed by the contents of the file followed by a couple of new lines followed by the next item which will be formatted the same way so this function returns two strings one is the string with a human readable description of what has changed and then the next one is the contents of all the files that have changed formatted in the way that i've describednow this method this higher level method that returns these two strings is called codebase check and the lower level one that it calls maybe should be called codebase check helper or something like that the codebase check helper no sorry the codebase check method is called for each of the code bases for each time that the application observes the following entry from the user the user has entered the text forward slash u that is the letter uniform followed by any amount of white space when the user enters this that prompts the application to run the codebase check for each codebase the method which calls the codebase check for each codebase will concatenate the codebase change descriptions in order of the codebases because the codebases have an order and it will prepend for each of those codebases a line saying codebase and the name of the codebase and then after the name of the codebase will be a carriage return and then the the a few lines describing what changed in the codebase and then an empty line and then the title of the next codebase and the chain list of changes for the next codebase it will also in the same order the codebase order it will concatenate the contents of all of the files from all of the codebases where the files have changed all of the file changes from the codebases so this will be a much longer string and that will be used to prepend to the next message that the user sends and there will also be a line outputted to the user printed saying the contents of these added files and updated files will be prepended to your next message that's what it will print to the userthe list of file changes that it's going to prepend should also include a line saying such and such files file has been deleted for each file that has gone\n\n\n#########\n\n#!/bin/env python\nimport atexit\nimport click\nimport datetime\nimport json\nimport logging\nimport os\nimport pyperclip\nimport re\nimport requests\nimport sys\nimport yaml\nfrom pathlib import Path\nfrom prompt_toolkit import PromptSession, HTML\nfrom prompt_toolkit.history import FileHistory\nfrom rich.console import Console\nfrom rich.logging import RichHandler\nfrom rich.markdown import Markdown\nfrom typing import Optional, List\nfrom xdg_base_dirs import xdg_config_home\nBASE = Path(xdg_config_home(), \"chatgpt-cli\")\nCONFIG_FILE = BASE / \"config.yaml\"\nHISTORY_FILE = BASE / \"history\"\nSAVE_FOLDER = BASE / \"session-history\"\nSAVE_FILE = (\n    \"chatgpt-session-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".json\"\n)\nOPENAI_BASE_ENDPOINT = os.environ.get(\n    \"OPENAI_BASE_ENDPOINT\", \"https://api.openai.com/v1\"\n)\nENV_VAR = \"OPENAI_API_KEY\"\n# Azure price is not accurate, it depends on your subscription\nPRICING_RATE = {\n    \"gpt-3.5-turbo\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-0125\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-1106\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-0613\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-16k\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-1106\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-0613\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-16k\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-4\": {\"prompt\": 0.03, \"completion\": 0.06},\n    \"gpt-4-0613\": {\"prompt\": 0.03, \"completion\": 0.06},\n    \"gpt-4-32k\": {\"prompt\": 0.06, \"completion\": 0.12},\n    \"gpt-4-32k-0613\": {\"prompt\": 0.06, \"completion\": 0.12},\n    \"gpt-4-1106-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n    \"gpt-4-0125-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n    \"gpt-4-turbo-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n}\nlogger = logging.getLogger(\"rich\")\nlogging.basicConfig(\n    level=\"INFO\",\n    format=\"%(message)s\",\n    handlers=[\n        RichHandler(show_time=False, show_level=False, show_path=False, markup=True)\n    ],\n)\n# Initialize the messages history list\n# It's mandatory to pass it at each API call in order to have a conversation\nmessages = []\n# Initialize the token counters\nprompt_tokens = 0\ncompletion_tokens = 0\n# Initialize the console\nconsole = Console()\nDEFAULT_CONFIG = {\n    \"supplier\": \"openai\",\n    \"api-key\": \"<INSERT YOUR  OPENAI API KEY HERE>\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"azure_endpoint\": \"https://xxxx.openai.azure.com/\",\n    \"azure_api_version\": \"2023-07-01-preview\",\n    \"azure_api_key\": \"<INSERT YOUR AZURE API KEY HERE>\",\n    \"azure_deployment_name\": \"gpt-35-turbo\",\n    \"azure_deployment_name_eb\": \"text-embedding-ada-002\",\n    \"temperature\": 1,\n    # 'max_tokens': 500,\n    \"markdown\": True,\n    \"easy_copy\": True,\n    \"non_interactive\": False,\n    \"json_mode\": False,\n    \"use_proxy\": False,\n    \"proxy\": \"socks5://127.0.0.1:2080\",\n}\ndef load_config(config_file: str) -> dict:\n    \"\"\"\n    Read a YAML config file and returns its content as a dictionary.\n    If the config file is missing, create one with default values.\n    If the config file is present but missing keys, populate them with defaults.\n    \"\"\"\n    # If the config file does not exist, create one with default configurations\n    if not Path(config_file).exists():\n        os.makedirs(os.path.dirname(config_file), exist_ok=True)\n        with open(config_file, \"w\", encoding=\"utf-8\") as file:\n            yaml.dump(DEFAULT_CONFIG, file, default_flow_style=False)\n        logger.info(f\"New config file initialized: [green bold]{config_file}\")\n    # Load existing config\n    with open(config_file, encoding=\"utf-8\") as file:\n        config = yaml.load(file, Loader=yaml.FullLoader)\n    # Update the loaded config with any default values that are missing\n    for key, value in DEFAULT_CONFIG.items():\n        if key not in config:\n            config[key] = value\n    return config\ndef load_history_data(history_file: str) -> dict:\n    \"\"\"\n    Read a session history json file and return its content\n    \"\"\"\n    with open(history_file, encoding=\"utf-8\") as file:\n        content = json.loads(file.read())\n    return content\ndef get_last_save_file() -> str:\n    \"\"\"\n    Return the timestamp of the last saved session\n    \"\"\"\n    files = [f for f in os.listdir(SAVE_FOLDER) if f.endswith(\".json\")]\n    if files:\n        ts = [f.replace(\"chatgpt-session-\", \"\").replace(\".json\", \"\") for f in files]\n        ts.sort()\n        return ts[-1]\n    return None\ndef create_save_folder() -> None:\n    \"\"\"\n    Create the session history folder if not exists\n    \"\"\"\n    if not os.path.exists(SAVE_FOLDER):\n        os.mkdir(SAVE_FOLDER)\ndef save_history(\n    model: str, messages: list, prompt_tokens: int, completion_tokens: int\n) -> None:\n    \"\"\"\n    Save the conversation history in JSON format\n    \"\"\"\n    with open(os.path.join(SAVE_FOLDER, SAVE_FILE), \"w\", encoding=\"utf-8\") as f:\n        json.dump(\n            {\n                \"model\": model,\n                \"messages\": messages,\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": completion_tokens,\n            },\n            f,\n            indent=4,\n            ensure_ascii=False,\n        )\ndef add_markdown_system_message() -> None:\n    \"\"\"\n    Try to force ChatGPT to always respond with well formatted code blocks and tables if markdown is enabled.\n    \"\"\"\n    instruction = \"Always use code blocks with the appropriate language tags. If asked for a table always format it using Markdown syntax.\"\n    messages.append({\"role\": \"system\", \"content\": instruction})\ndef calculate_expense(\n    prompt_tokens: int,\n    completion_tokens: int,\n    prompt_pricing: float,\n    completion_pricing: float,\n) -> float:\n    \"\"\"\n    Calculate the expense, given the number of tokens and the pricing rates\n    \"\"\"\n    expense = ((prompt_tokens / 1000) * prompt_pricing) + (\n        (completion_tokens / 1000) * completion_pricing\n    )\n    # Format to display in decimal notation rounded to 6 decimals\n    expense = \"{:.6f}\".format(round(expense, 6))\n    return expense\ndef display_expense(model: str) -> None:\n    \"\"\"\n    Given the model used, display total tokens used and estimated expense\n    \"\"\"\n    logger.info(\n        f\"\\nTotal tokens used: [green bold]{prompt_tokens + completion_tokens}\",\n        extra={\"highlighter\": None},\n    )\n    if model in PRICING_RATE:\n        total_expense = calculate_expense(\n            prompt_tokens,\n            completion_tokens,\n            PRICING_RATE[model][\"prompt\"],\n            PRICING_RATE[model][\"completion\"],\n        )\n        logger.info(\n            f\"Estimated expense: [green bold]${total_expense}\",\n            extra={\"highlighter\": None},\n        )\n    else:\n        logger.warning(\n            f\"[red bold]No expense estimate available for model {model}\",\n            extra={\"highlighter\": None},\n        )\ndef print_markdown(content: str, code_blocks: Optional[dict] = None):\n    \"\"\"\n    Print markdown formatted text to the terminal.\n    If code_blocks is present, label each code block with an integer and store in the code_blocks map.\n    \"\"\"\n    if code_blocks is None:\n        console.print(Markdown(content))\n        return\n    lines = content.split(\"\\n\")\n    code_block_id = 0 if code_blocks is None else 1 + max(code_blocks.keys(), default=0)\n    code_block_open = False\n    code_block_language = \"\"\n    code_block_content = []\n    regular_content = []\n    for line in lines:\n        if line.startswith(\"```\") and not code_block_open:\n            code_block_open = True\n            code_block_language = line.replace(\"```\", \"\").strip()\n            if regular_content:\n                console.print(Markdown(\"\\n\".join(regular_content)))\n                regular_content = []\n        elif line.startswith(\"```\") and code_block_open:\n            code_block_open = False\n            snippet_text = \"\\n\".join(code_block_content)\n            if code_blocks is not None:\n                code_blocks[code_block_id] = snippet_text\n            formatted_code_block = f\"```{code_block_language}\\n{snippet_text}\\n```\"\n            console.print(f\"Block {code_block_id}\", style=\"blue\", justify=\"right\")\n            console.print(Markdown(formatted_code_block))\n            code_block_id += 1\n            code_block_content = []\n            code_block_language = \"\"\n        elif code_block_open:\n            code_block_content.append(line)\n        else:\n            regular_content.append(line)\n    if code_block_open:  # uh-oh, the code block was never closed.\n        console.print(Markdown(\"\\n\".join(code_block_content)))\n    elif regular_content:  # If there's any remaining regular content, print it\n        console.print(Markdown(\"\\n\".join(regular_content)))\n###############################################################################\ndef get_size(contents: str) -> str:\n    \"\"\"\n    Get the size of the string in KB and format the output as a string.\n    \"\"\"\n    size = len(contents) / 1024\n    return f\"{size:.2f} KB\"\n###############################################################################\ndef load_codebase(base_path: str, extensions: List[str]) -> str:\n    \"\"\"\n    Concatenates the contents of files in the given directory and its subdirectories\n    that match the specified file extensions.\n    Parameters:\n    - base_path (str): The starting directory path to search for files.\n    - extensions (List[str]): A list of file extension strings to include (e.g., ['py', 'txt']).\n    Returns:\n    - str: A single string containing the concatenated contents of all matching files,\n           with headers indicating each file's path relative to base_path.\n    Raises:\n    - ValueError: If base_path does not exist or is not a directory.\n    - FileNotFoundError: If no matching files are found.\n    \"\"\"\n    # Verify the base path exists and is a directory\n    if not os.path.exists(base_path) or not os.path.isdir(base_path):\n        raise ValueError(f\"The path {base_path} does not exist or is not a directory.\")\n   \n    concatenated_contents = \"\"\n    matched_files_found = False\n    encodings = ['utf-8', 'cp1252', 'iso-8859-1']\n    # Walk through the directory and subdirectories\n    for root, dirs, files in os.walk(base_path):\n        for file_name in files:\n            if any(file_name.endswith(f\".{ext}\") for ext in extensions) or not(any(extensions)):\n                matched_files_found = True\n                file_path = Path(root) / file_name\n                relative_path = file_path.relative_to(base_path)\n                for encoding in encodings:\n                    try:\n                        with open(file_path, 'r', encoding=encoding) as file:\n                            contents = file.read()\n                            concatenated_contents += f\"### {relative_path} ###\\n{contents}\\n\\n\"\n                    except Exception as e:\n                        logger.warning(f\"Failed to open file {file_path} with encoding {encoding}: {e}\")\n    if not matched_files_found:\n        raise FileNotFoundError(\"No matching files found.\")\n    return concatenated_contents\n###############################################################################\ndef start_prompt(\n    session: PromptSession,\n    config: dict,\n    copyable_blocks: Optional[dict],\n    proxy: dict | None,\n) -> None:\n    \"\"\"\n    Ask the user for input, build the request and perform it\n    \"\"\"\n    # TODO: Refactor to avoid a global variables\n    global prompt_tokens, completion_tokens\n    message = \"\"\n    if config[\"non_interactive\"]:\n        message = sys.stdin.read()\n    else:\n        message = session.prompt(\n            HTML(f\"<b>[{prompt_tokens + completion_tokens}] >>> </b>\")\n        )\n    if message.lower().strip() == \"/q\":\n        # this is a bit strange to be raising exceptions for normal conditions\n        raise EOFError\n    if message.lower() == \"\":\n        raise KeyboardInterrupt\n    if config[\"easy_copy\"] and message.lower().startswith(\"/c\"):\n        # Use regex to find digits after /c or /copy\n        match = re.search(r\"^/c(?:opy)?\\s*(\\d+)\", message.lower())\n        if match:\n            block_id = int(match.group(1))\n            if block_id in copyable_blocks:\n                try:\n                    pyperclip.copy(copyable_blocks[block_id])\n                    logger.info(f\"Copied block {block_id} to clipboard\")\n                except pyperclip.PyperclipException:\n                    logger.error(\n                        \"Unable to perform the copy operation. Check https://pyperclip.readthedocs.io/en/latest/#not-implemented-error\"\n                    )\n            else:\n                logger.error(\n                    f\"No code block with ID {block_id} available\",\n                    extra={\"highlighter\": None},\n                )\n        elif messages:\n            pyperclip.copy(messages[-1][\"content\"])\n            logger.info(f\"Copied previous response to clipboard\")\n        raise KeyboardInterrupt\n    messages.append({\"role\": \"user\", \"content\": message})\n    if config[\"supplier\"] == \"azure\":\n        api_key = config[\"azure_api_key\"]\n        model = config[\"azure_deployment_name\"]\n        api_version = config[\"azure_api_version\"]\n        base_endpoint = config[\"azure_endpoint\"]\n    elif config[\"supplier\"] == \"openai\":\n        api_key = config[\"api-key\"]\n        model = config[\"model\"]\n        base_endpoint = OPENAI_BASE_ENDPOINT\n    else:\n        logger.error(\"Supplier must be either 'azure' or 'openai'\")\n    # Base body parameters\n    body = {\n        \"model\": model,\n        \"temperature\": config[\"temperature\"],\n        \"messages\": messages,\n    }\n    # Optional parameters\n    if \"max_tokens\" in config:\n        body[\"max_tokens\"] = config[\"max_tokens\"]\n    if config[\"json_mode\"]:\n        body[\"response_format\"] = {\"type\": \"json_object\"}\n    try:\n        if config[\"supplier\"] == \"azure\":\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"api-key\": api_key,\n            }\n            r = requests.post(\n                f\"{base_endpoint}/openai/deployments/{model}/chat/completions?api-version={api_version}\",\n                headers=headers,\n                json=body,\n                proxies=proxy,\n            )\n        elif config[\"supplier\"] == \"openai\":\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {api_key}\",\n            }\n            r = requests.post(\n                f\"{base_endpoint}/chat/completions\",\n                headers=headers,\n                json=body,\n                proxies=proxy,\n            )\n    except requests.ConnectionError:\n        logger.error(\n            \"[red bold]Connection error, try again...\", extra={\"highlighter\": None}\n        )\n        messages.pop()\n        raise KeyboardInterrupt\n    except requests.Timeout:\n        logger.error(\n            \"[red bold]Connection timed out, try again...\", extra={\"highlighter\": None}\n        )\n        messages.pop()\n        raise KeyboardInterrupt\n    match r.status_code:\n        case 200:\n            response = r.json()\n            message_response = response[\"choices\"][0][\"message\"]\n            usage_response = response[\"usage\"]\n            if not config[\"non_interactive\"]:\n                console.line()\n            if config[\"markdown\"]:\n                print_markdown(message_response[\"content\"].strip(), copyable_blocks)\n            else:\n                print(message_response[\"content\"].strip())\n            if not config[\"non_interactive\"]:\n                console.line()\n            # Update message history and token counters\n            messages.append(message_response)\n            prompt_tokens += usage_response[\"prompt_tokens\"]\n            completion_tokens += usage_response[\"completion_tokens\"]\n            save_history(model, messages, prompt_tokens, completion_tokens)\n            if config[\"non_interactive\"]:\n                # In non-interactive mode there is no looping back for a second prompt, you're done.\n                raise EOFError\n        case 400:\n            response = r.json()\n            if \"error\" in response:\n                if response[\"error\"][\"code\"] == \"context_length_exceeded\":\n                    logger.error(\n                        \"[red bold]Maximum context length exceeded\",\n                        extra={\"highlighter\": None},\n                    )\n                    raise EOFError\n                    # TODO: Develop a better strategy to manage this case\n            logger.error(\"[red bold]Invalid request\", extra={\"highlighter\": None})\n            raise EOFError\n        case 401:\n            logger.error(\"[red bold]Invalid API Key\", extra={\"highlighter\": None})\n            raise EOFError\n        case 429:\n            logger.error(\n                \"[red bold]Rate limit or maximum monthly limit exceeded\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case 500:\n            logger.error(\n                \"[red bold]Internal server error, check https://status.openai.com\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case 502 | 503:\n            logger.error(\n                \"[red bold]The server seems to be overloaded, try again\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case _:\n            logger.error(\n                f\"[red bold]Unknown error, status code {r.status_code}\",\n                extra={\"highlighter\": None},\n            )\n            logger.error(r.json(), extra={\"highlighter\": None})\n            raise EOFError\n###############################################################################\n@click.command()\n@click.option(\n    \"-s\",\n    \"--source\",\n    \"sources\",\n    type=click.Path(exists=True),\n    help=\"Pass an entire codebase to the model as context, from the specified location. \"\n        \"Repeat this option and its argument any number of times.\",\n    multiple=True,\n)\n@click.option(\n    \"-e\",\n    \"--file-extensions\",\n    \"file_extensions\",\n    required=False,\n    help=\"File name extensions of files to look at in the codebase, separated by commas without spaces, e.g. py,txt,md \"\n        \"Only use this option once, even for multiple codebases.\"\n)\n@click.option(\n    \"-c\",\n    \"--context\",\n    \"context\",\n    type=click.File(\"r\"),\n    help=\"Path to a context file\",\n    multiple=True,\n)\n@click.option(\"-k\", \"--key\", \"api_key\", help=\"Set the API Key\")\n@click.option(\"-m\", \"--model\", \"model\", help=\"Set the model\")\n@click.option(\n    \"-ml\", \"--multiline\", \"multiline\", is_flag=True, help=\"Use the multiline input mode\"\n)\n@click.option(\n    \"-r\",\n    \"--restore\",\n    \"restore\",\n    help=\"Restore a previous chat session (input format: YYYYMMDD-hhmmss or 'last')\",\n)\n@click.option(\n    \"-n\",\n    \"--non-interactive\",\n    \"non_interactive\",\n    is_flag=True,\n    help=\"Non interactive/command mode for piping\",\n)\n@click.option(\n    \"-j\", \"--json\", \"json_mode\", is_flag=True, help=\"Activate json response mode\"\n)\ndef main(\n    sources, context, api_key, model, multiline, restore, non_interactive, json_mode,\n    file_extensions\n) -> None:\n    # If non interactive suppress the logging messages\n    if non_interactive:\n        logger.setLevel(\"ERROR\")\n    logger.info(\"[bold]ChatGPT CLI\", extra={\"highlighter\": None})\n    history = FileHistory(HISTORY_FILE)\n    if multiline:\n        session = PromptSession(history=history, multiline=True)\n    else:\n        session = PromptSession(history=history)\n    try:\n        config = load_config(CONFIG_FILE)\n    except FileNotFoundError:\n        logger.error(\n            \"[red bold]Configuration file not found\", extra={\"highlighter\": None}\n        )\n        sys.exit(1)\n    create_save_folder()\n    # check proxy setting\n    if config[\"use_proxy\"]:\n        proxy = {\"http\": config[\"proxy\"], \"https\": config[\"proxy\"]}\n    else:\n        proxy = None\n    # Order of precedence for API Key configuration:\n    # Command line option > Environment variable > Configuration file\n    # If the environment variable is set overwrite the configuration\n    if os.environ.get(ENV_VAR):\n        config[\"api-key\"] = os.environ[ENV_VAR].strip()\n    # If the --key command line argument is used overwrite the configuration\n    if api_key:\n        if config[\"supplier\"] == \"openai\":\n            config[\"api-key\"] = api_key.strip()\n        else:\n            config[\"azure_api_key\"] = api_key.strip()\n    # If the --model command line argument is used overwrite the configuration\n    if model:\n        if config[\"supplier\"] == \"openai\":\n            config[\"model\"] = model.strip()\n        else:\n            config[\"azure_deployment_name\"] = model.strip()\n    config[\"non_interactive\"] = non_interactive\n    # Do not emit markdown in this case; ctrl character formatting interferes in several contexts including json\n    # output.\n    if config[\"non_interactive\"]:\n        config[\"markdown\"] = False\n    config[\"json_mode\"] = json_mode\n    copyable_blocks = {} if config[\"easy_copy\"] else None\n    if config[\"supplier\"] == \"azure\":\n        model = config[\"azure_deployment_name\"]\n    elif config[\"supplier\"] == \"openai\":\n        model = config[\"model\"]\n    # Run the display expense function when exiting the script\n    atexit.register(display_expense, model=model)\n    logger.info(\n        f\"Supplier: [green bold]{config['supplier']}\", extra={\"highlighter\": None}\n    )\n    logger.info(f\"Model in use: [green bold]{model}\", extra={\"highlighter\": None})\n    # Add the system message for code blocks in case markdown is enabled in the config file\n    if config[\"markdown\"]:\n        add_markdown_system_message()\n    # Source code location from command line option\n    if sources:\n        for source in sources:\n            logger.info(\n                f\"Codebase location: [green bold]{source}\\n\",\n                extra={\"highlighter\": None},\n            )\n            logger.info(\n                \"The entire codebase will be prepended to your first message.\"\n            )\n            extensions = []\n            if file_extensions != ():\n                logger.info(\n                    f\"Looking only at source files with extensions: [green bold]{file_extensions}\\n\",\n                    extra={\"highlighter\": None},\n                )\n                extensions = [ext.strip() for ext in file_extensions.split(\",\")]\n            try:\n                codebase = load_codebase(source, extensions)\n                messages.append({\"role\": \"system\", \"content\": codebase})\n               \n                # Show the user how big the entire codebase is, in kb.\n                logger.info(\n                    f\"Codebase size: [green bold]{get_size(codebase)}\\n\",\n                )\n            except FileNotFoundError as e:\n                print(f\"Error reading codebase: {e}\")\n    # Context from the command line option\n    if context:\n        for c in context:\n            logger.info(\n                f\"Context file: [green bold]{c.name}\", extra={\"highlighter\": None}\n            )\n            messages.append({\"role\": \"system\", \"content\": c.read().strip()})\n    # Restore a previous session\n    if restore:\n        if restore == \"last\":\n            last_session = get_last_save_file()\n            restore_file = f\"chatgpt-session-{last_session}.json\"\n        else:\n            restore_file = f\"chatgpt-session-{restore}.json\"\n        try:\n            global prompt_tokens, completion_tokens\n            # If this feature is used --context is cleared\n            messages.clear()\n            history_data = load_history_data(os.path.join(SAVE_FOLDER, restore_file))\n            for message in history_data[\"messages\"]:\n                messages.append(message)\n            prompt_tokens += history_data[\"prompt_tokens\"]\n            completion_tokens += history_data[\"completion_tokens\"]\n            logger.info(\n                f\"Restored session: [bold green]{restore}\",\n                extra={\"highlighter\": None},\n            )\n        except FileNotFoundError:\n            logger.error(\n                f\"[red bold]File {restore_file} not found\", extra={\"highlighter\": None}\n            )\n    if json_mode:\n        logger.info(\n            \"JSON response mode is active. Your message should contain the [bold]'json'[/bold] word.\",\n            extra={\"highlighter\": None},\n        )\n    if not non_interactive:\n        console.rule()\n    while True:\n        try:\n            start_prompt(session, config, copyable_blocks, proxy)\n        except KeyboardInterrupt:\n            continue\n        except EOFError:\n            break\nif __name__ == \"__main__\":\n    main()\n"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "Here's the modified code with the requested changes:\n\n```python\n#!/bin/env python\nimport atexit\nimport click\nimport datetime\nimport json\nimport logging\nimport os\nimport pyperclip\nimport re\nimport requests\nimport sys\nimport yaml\nfrom pathlib import Path\nfrom prompt_toolkit import PromptSession, HTML\nfrom prompt_toolkit.history import FileHistory\nfrom rich.console import Console\nfrom rich.logging import RichHandler\nfrom rich.markdown import Markdown\nfrom typing import Optional, List\nfrom xdg_base_dirs import xdg_config_home\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nBASE = Path(xdg_config_home(), \"chatgpt-cli\")\nCONFIG_FILE = BASE / \"config.yaml\"\nHISTORY_FILE = BASE / \"history\"\nSAVE_FOLDER = BASE / \"session-history\"\nSAVE_FILE = (\n    \"chatgpt-session-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".json\"\n)\nOPENAI_BASE_ENDPOINT = os.environ.get(\n    \"OPENAI_BASE_ENDPOINT\", \"https://api.openai.com/v1\"\n)\nENV_VAR = \"OPENAI_API_KEY\"\n# Azure price is not accurate, it depends on your subscription\nPRICING_RATE = {\n    \"gpt-3.5-turbo\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-0125\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-1106\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-0613\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-3.5-turbo-16k\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-1106\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-0613\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-35-turbo-16k\": {\"prompt\": 0.0005, \"completion\": 0.0015},\n    \"gpt-4\": {\"prompt\": 0.03, \"completion\": 0.06},\n    \"gpt-4-0613\": {\"prompt\": 0.03, \"completion\": 0.06},\n    \"gpt-4-32k\": {\"prompt\": 0.06, \"completion\": 0.12},\n    \"gpt-4-32k-0613\": {\"prompt\": 0.06, \"completion\": 0.12},\n    \"gpt-4-1106-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n    \"gpt-4-0125-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n    \"gpt-4-turbo-preview\": {\"prompt\": 0.01, \"completion\": 0.03},\n}\nlogger = logging.getLogger(\"rich\")\nlogging.basicConfig(\n    level=\"INFO\",\n    format=\"%(message)s\",\n    handlers=[\n        RichHandler(show_time=False, show_level=False, show_path=False, markup=True)\n    ],\n)\n# Initialize the messages history list\n# It's mandatory to pass it at each API call in order to have a conversation\nmessages = []\n# Initialize the token counters\nprompt_tokens = 0\ncompletion_tokens = 0\n# Initialize the console\nconsole = Console()\nDEFAULT_CONFIG = {\n    \"supplier\": \"openai\",\n    \"api-key\": \"<INSERT YOUR  OPENAI API KEY HERE>\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"azure_endpoint"
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Keep going"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "Here's the continuation of the modified code:\n\n```python\n    \"azure_endpoint\": \"https://xxxx.openai.azure.com/\",\n    \"azure_api_version\": \"2023-07-01-preview\",\n    \"azure_api_key\": \"<INSERT YOUR AZURE API KEY HERE>\",\n    \"azure_deployment_name\": \"gpt-35-turbo\",\n    \"azure_deployment_name_eb\": \"text-embedding-ada-002\",\n    \"temperature\": 1,\n    # 'max_tokens': 500,\n    \"markdown\": True,\n    \"easy_copy\": True,\n    \"non_interactive\": False,\n    \"json_mode\": False,\n    \"use_proxy\": False,\n    \"proxy\": \"socks5://127.0.0.1:2080\",\n    \"codebases\": [],\n}\n\n@dataclass\nclass CodebaseState:\n    location: str\n    file_extensions: List[str]\n    files: dict[str, datetime]\n\n@dataclass\nclass CodebaseTransformation:\n    additions: set[str]\n    deletions: set[str]\n    updates: set[tuple[str, datetime]]\n\ndef load_config(config_file: str) -> dict:\n    \"\"\"\n    Read a YAML config file and returns its content as a dictionary.\n    If the config file is missing, create one with default values.\n    If the config file is present but missing keys, populate them with defaults.\n    \"\"\"\n    # If the config file does not exist, create one with default configurations\n    if not Path(config_file).exists():\n        os.makedirs(os.path.dirname(config_file), exist_ok=True)\n        with open(config_file, \"w\", encoding=\"utf-8\") as file:\n            yaml.dump(DEFAULT_CONFIG, file, default_flow_style=False)\n        logger.info(f\"New config file initialized: [green bold]{config_file}\")\n    # Load existing config\n    with open(config_file, encoding=\"utf-8\") as file:\n        config = yaml.load(file, Loader=yaml.FullLoader)\n    # Update the loaded config with any default values that are missing\n    for key, value in DEFAULT_CONFIG.items():\n        if key not in config:\n            config[key] = value\n    return config\n\ndef load_history_data(history_file: str) -> dict:\n    \"\"\"\n    Read a session history json file and return its content\n    \"\"\"\n    with open(history_file, encoding=\"utf-8\") as file:\n        content = json.loads(file.read())\n    return content\n\ndef get_last_save_file() -> str:\n    \"\"\"\n    Return the timestamp of the last saved session\n    \"\"\"\n    files = [f for f in os.listdir(SAVE_FOLDER) if f.endswith(\".json\")]\n    if files:\n        ts = [f.replace(\"chatgpt-session-\", \"\").replace(\".json\", \"\") for f in files]\n        ts.sort()\n        return ts[-1]\n    return None\n\ndef create_save_folder() -> None:\n    \"\"\"\n    Create the session history folder if not exists\n    \"\"\"\n    if not os.path.exists(SAVE_FOLDER):\n        os.mkdir(SAVE_FOLDER)\n\ndef save_history(\n    model: str, messages: list, prompt_tokens: int, completion_tokens: int\n) -> None:\n    \"\"\"\n    Save the conversation history in JSON format\n    \"\"\"\n    with open(os.path.join(SAVE_FOLDER, SAVE_FILE), \"w\", encoding=\"utf-8\") as f:\n        json.dump(\n            {\n                \"model\": model,\n                \"messages\": messages,\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": completion_tokens,\n            },\n            f,\n            indent=4,\n            ensure_ascii=False,\n        )\n\ndef add_markdown_system_message() -> None:\n    \"\"\"\n    Try to force ChatGPT to always respond with well formatted code blocks and tables if markdown is enabled.\n    \"\"\"\n    instruction = \"Always use code blocks with the appropriate language tags. If asked for a table always format it using Markdown syntax.\"\n    messages.append({\"role\": \"system\", \"content\": instruction})\n\ndef calculate_expense(\n    prompt_tokens: int,\n    completion_tokens: int,\n    prompt_pricing: float,\n    completion_pricing: float,\n) -> float:\n    \"\"\"\n    Calculate the expense, given the number of tokens and the pricing rates\n    \"\"\"\n    expense = ((prompt_tokens / 1000) * prompt_pricing) + (\n        (completion_tokens / 1000) * completion_pricing\n    )\n    # Format to display in decimal notation rounded to 6 decimals\n    expense = \"{:.6f}\".format(round(expense, 6))\n    return expense\n\ndef display_expense(model: str) -> None:\n    \"\"\"\n    Given the model used, display total tokens used and estimated expense\n    \"\"\"\n    logger.info(\n        f\"\\nTotal tokens used: [green bold]{prompt_tokens + completion_tokens}\",\n        extra={\"highlighter\": None},\n    )\n    if model in PRICING_RATE:\n        total_expense = calculate_expense(\n            prompt_tokens,\n            completion_tokens,\n            PRICING_RATE[model][\"prompt\"],\n            PRICING_RATE[model][\"completion\"],\n        )\n        logger.info(\n            f\"Estimated expense: [green bold]${total_expense}\",\n            extra={\"highlighter\": None},\n        )\n    else:\n        logger.warning(\n            f\"[red bold]No expense estimate available for model {model}\",\n            extra={\"highlighter\": None},\n        )\n\ndef print_markdown(content: str, code_blocks: Optional[dict] = None):\n    \"\"\"\n    Print markdown formatted text to the terminal.\n    If code_blocks is present, label each code block with an integer and store in the code_blocks map.\n    \"\"\"\n    if code_blocks is None:\n        console.print(Markdown(content))\n        return\n    lines = content.split(\"\\n\")\n    code_block_id = 0 if code_blocks is None else 1 + max(code_blocks.keys(), default=0)\n    code_block_open = False\n    code_block_language = \"\"\n    code_block_content = []\n    regular_content = []\n    for line in lines:\n        if line.startswith(\"```\") and not code_block_open:\n            code_block_open = True\n            code_block_language = line.replace(\"```\", \"\").strip()\n            if regular_content:\n                console.print(Markdown(\"\\n\".join(regular_content)))\n                regular_content = []\n        elif line.startswith(\"```\") and code_block_open:\n            code_block_open = False\n            snippet_text = \"\\n\".join(code_block_content)\n            if code_blocks is not None:\n                code_blocks[code_block_id] = snippet_text\n            formatted_code_block = f\"```{code_block_language}\\n{snippet_text}\\n```\"\n            console.print(f\"Block {code_block_id}\", style=\"blue\", justify=\"right\")\n            console.print(Markdown(formatted_code_block))\n            code_block_id += 1\n            code_block_content = []\n            code_block_language = \"\"\n        elif code_block_open:\n            code_block_content.append(line)\n        else:\n            regular_content.append(line)\n    if code_block_open:  # uh-oh, the code block was never closed.\n        console.print(Markdown(\"\\n\".join(code_block_content)))\n    elif regular_content:  # If there's any remaining regular content, print it\n        console.print(Markdown(\"\\n\".join(regular_content)))\n\n###############################################################################\ndef get_size(contents: str) -> str:\n    \"\"\"\n    Get the size of the string in KB and format the output as a string.\n    \"\"\"\n    size = len(contents) / 1024\n    return f\"{size:.2f} KB\"\n\n###############################################################################\ndef load_codebase(base_path: str, extensions: List[str]) -> str:\n    \"\"\"\n    Concatenates the contents of files in the given directory and its subdirectories\n    that match the specified file extensions.\n    Parameters:\n    - base_path (str): The starting directory path to search for files.\n    - extensions (List[str]): A list of file extension strings to include (e.g., ['py', 'txt']).\n    Returns:\n    - str: A single string containing the concatenated contents of all matching files,\n           with headers indicating each file's path relative to base_path.\n    Raises:\n    - ValueError: If base_path does not exist or is not a directory.\n    - FileNotFoundError: If no matching files are found.\n    \"\"\"\n    # Verify the base path exists and is a directory\n    if not os.path.exists(base_path) or not os.path.isdir(base_path):\n        raise ValueError(f\"The path {base_path} does not exist or is not a directory.\")\n   \n    concatenated_contents = \"\"\n    matched_files_found = False\n    encodings = ['utf-8', 'cp1252', 'iso-8859-1']\n    # Walk through the directory and subdirectories\n    for root, dirs, files in os.walk(base_path):\n        for file_name in files:\n            if any(file_name.endswith(f\".{ext}\") for ext in extensions) or not(any(extensions)):\n                matched_files_found = True\n                file_path = Path(root) / file_name\n                relative_path = file_path.relative_to(base_path)\n                for encoding in encodings:\n                    try:\n                        with open(file_path, 'r', encoding=encoding) as file:\n                            contents = file.read()\n                            concatenated_contents += f\"### {relative_path} ###\\n{contents}\\n\\n\"\n                    except Exception as e:\n                        logger.warning(f\"Failed to open file {file_path} with encoding {encoding}: {e}\")\n    if not matched_files_found:\n        raise FileNotFoundError(\"No matching files found.\")\n    return concatenated_contents\n\n###############################################################################\ndef codebase_check_helper(codebase_location: str, file_extensions: List[str], codebase_state: CodebaseState) -> tuple[CodebaseTransformation, set[tuple[str, str]]]:\n    \"\"\"\n    Checks the codebase for changes and returns a CodebaseTransformation object representing the changes\n    and a set of pairs of relative paths and contents of added or modified files.\n    \"\"\"\n    current_files = {}\n    for root, dirs, files in os.walk(codebase_location):\n        for file_name in files:\n            if any(file_name.endswith(f\".{ext}\") for ext in file_extensions):\n                file_path = Path(root) / file_name\n                relative_path = str(file_path.relative_to(codebase_location))\n                current_files[relative_path] = datetime.fromtimestamp(os.path.getmtime(file_path))\n\n    additions = set(current_files.keys()) - set(codebase_state.files.keys())\n    deletions = set(codebase_state.files.keys()) - set(current_files.keys())\n    updates = set()\n    for relative_path, modified_time in current_files.items():\n        if relative_path in codebase_state.files and modified_time > codebase_state.files[relative_path]:\n            updates.add((relative_path, modified_time))\n\n    transformation = CodebaseTransformation(additions, deletions, updates)\n\n    changed_files = set()\n    for relative_path in additions | {path for path, _ in updates}:\n        file_path = Path(codebase_location) / relative_path\n        with open(file_path, 'r') as file:\n            contents = file.read()\n            changed_files.add((relative_path, contents))\n\n    return transformation, changed_files\n\ndef codebase_check(codebase_location: str, codebase_state: CodebaseState) -> tuple[str, str]:\n    \"\"\"\n    Checks the codebase for changes and returns a human-readable description of the changes\n    and a string containing the contents of added or modified files.\n    \"\"\"\n    transformation, changed_files = codebase_check_helper(codebase_location, codebase_state.file_extensions, codebase_state)\n\n    new_codebase_state = CodebaseState(\n        codebase_state.location,\n        codebase_state.file_extensions,\n        {**codebase_state.files, **{path: modified_time for path, modified_time in transformation.updates}}\n    )\n    for path in transformation.additions:\n        new_codebase_state.files[path] = datetime.now()\n    for path in transformation.deletions:\n        del new_codebase_state.files[path]\n\n    change_description = transformation.to_text()\n    file_contents = \"\\n\\n\".join(f\"Contents of file {path}:\\n{contents}\" for path, contents in changed_files)\n\n    return change_description, file_contents\n\ndef start_prompt(\n    session: PromptSession,\n    config: dict,\n    copyable_blocks: Optional[dict],\n    proxy: dict | None,\n) -> None:\n    \"\"\"\n    Ask the user for input, build the request and perform it\n    \"\"\"\n    # TODO: Refactor to avoid a global variables\n    global prompt_tokens, completion_tokens\n    message = \"\"\n    if config[\"non_interactive\"]:\n        message = sys.stdin.read()\n    else:\n        message = session.prompt(\n            HTML(f\"<b>[{prompt_tokens + completion_tokens}] >>> </b>\")\n        )\n    if message.lower().strip() == \"/q\":\n        # this is a bit strange to be raising exceptions for normal conditions\n        raise EOFError\n    if message.lower() == \"\":\n        raise KeyboardInterrupt\n    if message.lower().startswith(\"/u\"):\n        change_descriptions = []\n        file_contents = []\n        for codebase_state in config[\"codebases\"]:\n            description, contents = codebase_check(codebase_state.location, codebase_state)\n            if description:\n                change_descriptions.append(f\"Codebase {codebase_state.location}:\\n{description}\\n\")\n                file_contents.append(contents)\n        if change_descriptions:\n            logger.info(\"The following changes were detected in the codebases:\")\n            logger.info(\"\\n\".join(change_descriptions))\n            logger.info(\"The contents of these added files and updated files will be prepended to your next message.\")\n            messages.append({\"role\": \"system\", \"content\": \"\\n\\n\".join(file_contents)})\n        else:\n            logger.info(\"No changes detected in the codebases.\")\n        raise KeyboardInterrupt\n    if config[\"easy_copy\"] and message.lower().startswith(\"/c\"):\n        # Use regex to find digits after /c or /copy\n        match = re.search(r\"^/c(?:opy)?\\s*(\\d+)\", message.lower())\n        if match:\n            block_id = int(match.group(1))\n            if block_id in copyable_blocks:\n                try:\n                    pyperclip.copy(copyable_blocks[block_id])\n                    logger.info(f\"Copied block {block_id} to clipboard\")\n                except pyperclip.PyperclipException:\n                    logger.error(\n                        \"Unable to perform the copy operation. Check https://pyperclip.readthedocs.io/en/latest/#not-implemented-error\"\n                    )\n            else:\n                logger.error(\n                    f\"No code block with ID {block_id} available\",\n                    extra={\"highlighter\": None},\n                )\n        elif messages:\n            pyperclip.copy(messages[-1][\"content\"])\n            logger.info(f\"Copied previous response to clipboard\")\n        raise KeyboardInterrupt\n    messages.append({\"role\": \"user\", \"content\": message})\n    if config[\"supplier\"] == \"azure\":\n        api_key = config[\"azure_api_key\"]\n        model = config[\"azure_"
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Keep going"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "text",
                    "text": "deployment_name\"]\n        api_version = config[\"azure_api_version\"]\n        base_endpoint = config[\"azure_endpoint\"]\n    elif config[\"supplier\"] == \"openai\":\n        api_key = config[\"api-key\"]\n        model = config[\"model\"]\n        base_endpoint = OPENAI_BASE_ENDPOINT\n    else:\n        logger.error(\"Supplier must be either 'azure' or 'openai'\")\n    # Base body parameters\n    body = {\n        \"model\": model,\n        \"temperature\": config[\"temperature\"],\n        \"messages\": messages,\n    }\n    # Optional parameters\n    if \"max_tokens\" in config:\n        body[\"max_tokens\"] = config[\"max_tokens\"]\n    if config[\"json_mode\"]:\n        body[\"response_format\"] = {\"type\": \"json_object\"}\n    try:\n        if config[\"supplier\"] == \"azure\":\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"api-key\": api_key,\n            }\n            r = requests.post(\n                f\"{base_endpoint}/openai/deployments/{model}/chat/completions?api-version={api_version}\",\n                headers=headers,\n                json=body,\n                proxies=proxy,\n            )\n        elif config[\"supplier\"] == \"openai\":\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {api_key}\",\n            }\n            r = requests.post(\n                f\"{base_endpoint}/chat/completions\",\n                headers=headers,\n                json=body,\n                proxies=proxy,\n            )\n    except requests.ConnectionError:\n        logger.error(\n            \"[red bold]Connection error, try again...\", extra={\"highlighter\": None}\n        )\n        messages.pop()\n        raise KeyboardInterrupt\n    except requests.Timeout:\n        logger.error(\n            \"[red bold]Connection timed out, try again...\", extra={\"highlighter\": None}\n        )\n        messages.pop()\n        raise KeyboardInterrupt\n    match r.status_code:\n        case 200:\n            response = r.json()\n            message_response = response[\"choices\"][0][\"message\"]\n            usage_response = response[\"usage\"]\n            if not config[\"non_interactive\"]:\n                console.line()\n            if config[\"markdown\"]:\n                print_markdown(message_response[\"content\"].strip(), copyable_blocks)\n            else:\n                print(message_response[\"content\"].strip())\n            if not config[\"non_interactive\"]:\n                console.line()\n            # Update message history and token counters\n            messages.append(message_response)\n            prompt_tokens += usage_response[\"prompt_tokens\"]\n            completion_tokens += usage_response[\"completion_tokens\"]\n            save_history(model, messages, prompt_tokens, completion_tokens)\n            if config[\"non_interactive\"]:\n                # In non-interactive mode there is no looping back for a second prompt, you're done.\n                raise EOFError\n        case 400:\n            response = r.json()\n            if \"error\" in response:\n                if response[\"error\"][\"code\"] == \"context_length_exceeded\":\n                    logger.error(\n                        \"[red bold]Maximum context length exceeded\",\n                        extra={\"highlighter\": None},\n                    )\n                    raise EOFError\n                    # TODO: Develop a better strategy to manage this case\n            logger.error(\"[red bold]Invalid request\", extra={\"highlighter\": None})\n            raise EOFError\n        case 401:\n            logger.error(\"[red bold]Invalid API Key\", extra={\"highlighter\": None})\n            raise EOFError\n        case 429:\n            logger.error(\n                \"[red bold]Rate limit or maximum monthly limit exceeded\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case 500:\n            logger.error(\n                \"[red bold]Internal server error, check https://status.openai.com\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case 502 | 503:\n            logger.error(\n                \"[red bold]The server seems to be overloaded, try again\",\n                extra={\"highlighter\": None},\n            )\n            messages.pop()\n            raise KeyboardInterrupt\n        case _:\n            logger.error(\n                f\"[red bold]Unknown error, status code {r.status_code}\",\n                extra={\"highlighter\": None},\n            )\n            logger.error(r.json(), extra={\"highlighter\": None})\n            raise EOFError\n\n###############################################################################\n@click.command()\n@click.option(\n    \"-s\",\n    \"--source\",\n    \"sources\",\n    type=click.Path(exists=True),\n    help=\"Pass an entire codebase to the model as context, from the specified location. \"\n        \"Repeat this option and its argument any number of times.\",\n    multiple=True,\n)\n@click.option(\n    \"-e\",\n    \"--file-extensions\",\n    \"file_extensions\",\n    required=False,\n    help=\"File name extensions of files to look at in the codebase, separated by commas without spaces, e.g. py,txt,md \"\n        \"Only use this option once, even for multiple codebases.\"\n)\n@click.option(\n    \"-c\",\n    \"--context\",\n    \"context\",\n    type=click.File(\"r\"),\n    help=\"Path to a context file\",\n    multiple=True,\n)\n@click.option(\"-k\", \"--key\", \"api_key\", help=\"Set the API Key\")\n@click.option(\"-m\", \"--model\", \"model\", help=\"Set the model\")\n@click.option(\n    \"-ml\", \"--multiline\", \"multiline\", is_flag=True, help=\"Use the multiline input mode\"\n)\n@click.option(\n    \"-r\",\n    \"--restore\",\n    \"restore\",\n    help=\"Restore a previous chat session (input format: YYYYMMDD-hhmmss or 'last')\",\n)\n@click.option(\n    \"-n\",\n    \"--non-interactive\",\n    \"non_interactive\",\n    is_flag=True,\n    help=\"Non interactive/command mode for piping\",\n)\n@click.option(\n    \"-j\", \"--json\", \"json_mode\", is_flag=True, help=\"Activate json response mode\"\n)\ndef main(\n    sources, context, api_key, model, multiline, restore, non_interactive, json_mode,\n    file_extensions\n) -> None:\n    # If non interactive suppress the logging messages\n    if non_interactive:\n        logger.setLevel(\"ERROR\")\n    logger.info(\"[bold]ChatGPT CLI\", extra={\"highlighter\": None})\n    history = FileHistory(HISTORY_FILE)\n    if multiline:\n        session = PromptSession(history=history, multiline=True)\n    else:\n        session = PromptSession(history=history)\n    try:\n        config = load_config(CONFIG_FILE)\n    except FileNotFoundError:\n        logger.error(\n            \"[red bold]Configuration file not found\", extra={\"highlighter\": None}\n        )\n        sys.exit(1)\n    create_save_folder()\n    # check proxy setting\n    if config[\"use_proxy\"]:\n        proxy = {\"http\": config[\"proxy\"], \"https\": config[\"proxy\"]}\n    else:\n        proxy = None\n    # Order of precedence for API Key configuration:\n    # Command line option > Environment variable > Configuration file\n    # If the environment variable is set overwrite the configuration\n    if os.environ.get(ENV_VAR):\n        config[\"api-key\"] = os.environ[ENV_VAR].strip()\n    # If the --key command line argument is used overwrite the configuration\n    if api_key:\n        if config[\"supplier\"] == \"openai\":\n            config[\"api-key\"] = api_key.strip()\n        else:\n            config[\"azure_api_key\"] = api_key.strip()\n    # If the --model command line argument is used overwrite the configuration\n    if model:\n        if config[\"supplier\"] == \"openai\":\n            config[\"model\"] = model.strip()\n        else:\n            config[\"azure_deployment_name\"] = model.strip()\n    config[\"non_interactive\"] = non_interactive\n    # Do not emit markdown in this case; ctrl character formatting interferes in several contexts including json\n    # output.\n    if config[\"non_interactive\"]:\n        config[\"markdown\"] = False\n    config[\"json_mode\"] = json_mode\n    copyable_blocks = {} if config[\"easy_copy\"] else None\n    if config[\"supplier\"] == \"azure\":\n        model = config[\"azure_deployment_name\"]\n    elif config[\"supplier\"] == \"openai\":\n        model = config[\"model\"]\n    # Run the display expense function when exiting the script\n    atexit.register(display_expense, model=model)\n    logger.info(\n        f\"Supplier: [green bold]{config['supplier']}\", extra={\"highlighter\": None}\n    )\n    logger.info(f\"Model in use: [green bold]{model}\", extra={\"highlighter\": None})\n    # Add the system message for code blocks in case markdown is enabled in the config file\n    if config[\"markdown\"]:\n        add_markdown_system_message()\n    # Source code location from command line option\n    if sources:\n        config[\"codebases\"] = []\n        extensions = []\n        if file_extensions != ():\n            logger.info(\n                f\"Looking only at source files with extensions: [green bold]{file_extensions}\\n\",\n                extra={\"highlighter\": None},\n            )\n            extensions = [ext.strip() for ext in file_extensions.split(\",\")]\n        for source in sources:\n            logger.info(\n                f\"Codebase location: [green bold]{source}\\n\",\n                extra={\"highlighter\": None},\n            )\n            logger.info(\n                \"The entire codebase will be prepended to your first message.\"\n            )\n            try:\n                codebase = load_codebase(source, extensions)\n                messages.append({\"role\": \"system\", \"content\": codebase})\n               \n                # Show the user how big the entire codebase is, in kb.\n                logger.info(\n                    f\"Codebase size: [green bold]{get_size(codebase)}\\n\",\n                )\n                config[\"codebases\"].append(CodebaseState(source, extensions, {}))\n            except FileNotFoundError as e:\n                print(f\"Error reading codebase: {e}\")\n    # Context from the command line option\n    if context:\n        for c in context:\n            logger.info(\n                f\"Context file: [green bold]{c.name}\", extra={\"highlighter\": None}\n            )\n            messages.append({\"role\": \"system\", \"content\": c.read().strip()})\n    # Restore a previous session\n    if restore:\n        if restore == \"last\":\n            last_session = get_last_save_file()\n            restore_file = f\"chatgpt-session-{last_session}.json\"\n        else:\n            restore_file = f\"chatgpt-session-{restore}.json\"\n        try:\n            global prompt_tokens, completion_tokens\n            # If this feature is used --context is cleared\n            messages.clear()\n            history_data = load_history_data(os.path.join(SAVE_FOLDER, restore_file))\n            for message in history_data[\"messages\"]:\n                messages.append(message)\n            prompt_tokens += history_data[\"prompt_tokens\"]\n            completion_tokens += history_data[\"completion_tokens\"]\n            logger.info(\n                f\"Restored session: [bold green]{restore}\",\n                extra={\"highlighter\": None},\n            )\n        except FileNotFoundError:\n            logger.error(\n                f\"[red bold]File {restore_file} not found\", extra={\"highlighter\": None}\n            )\n    if json_mode:\n        logger.info(\n            \"JSON response mode is active. Your message should contain the [bold]'json'[/bold] word.\",\n            extra={\"highlighter\": None},\n        )\n    if not non_interactive:\n        console.rule()\n    while True:\n        try:\n            start_prompt(session, config, copyable_blocks, proxy)\n        except KeyboardInterrupt:\n            continue\n        except EOFError:\n            break\n\nif __name__ == \"__main__\":\n    main()\n```\n\nAnd here's the additional code file for the `CodebaseState` and `CodebaseTransformation` classes:\n\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Set, Tuple\n\n@dataclass\nclass CodebaseState:\n    location: str\n    file_extensions: List[str]\n    files: dict[str, datetime]\n\n@dataclass\nclass CodebaseTransformation:\n    additions: Set[str]\n    deletions: Set[str]\n    updates: Set[Tuple[str, datetime]]\n\n    def to_text(self) -> str:\n        lines = []\n        if self.additions:\n            lines.append(\"Added files:\")\n            for path in self.additions:\n                lines.append(f\"- {path}\")\n        if self.deletions:\n            lines.append(\"Deleted files:\")\n            for path in self.deletions:\n                lines.append(f\"- {path}\")\n        if self.updates:\n            lines.append(\"Updated files:\")\n            for path, modified_time in self.updates:\n                lines.append(f\"- {path} (modified at {modified_time})\")\n        return \"\\n\".join(lines)\n```\n\nThese changes implement the codebase watching functionality as described. The `/u` command checks for changes in the watched codebases and prepends the contents of added or modified files to the next message. The codebase state is treated as an immutable object, and changes are represented using the `CodebaseTransformation` class. The `codebase_check_helper` function identifies the changes, and the `codebase_check` function returns a human-readable description of the changes and the contents of the changed files."
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Now can you make a diff on your previous code to use named types in place of all tuples and some strings where feasible? I want type aliases like this:\n\nfrom typing import NewType \n\nDescription = NewType('Description', str) \n\ndef print_description(desc: Description): print(desc)"
                }
            ]
        }
    ]
)
print(message.content)